{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file shows steps to construct synthetic data using the method shown in the paper. <br>\n",
    "We first import packages and set a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"models\")\n",
    "sys.path.append(\"AIF360/\")\n",
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "from aif360.datasets import StandardDataset\n",
    "#from fairness_metrics.tot_metrics import TPR, TNR\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "#from compas_model import get_evaluation\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define label column. In this example, I have 10000 0's and 10000 1's for the label column. 0 is the negative outcome and 1 is the positive outcome. <br>\n",
    "We also need to define the sensitive attribute value and the correlation between sensitive attribute and the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [0] * 10000\n",
    "Y.extend([1] * 10000)\n",
    "# S here is the sensitive attribute where 0 is the unprivileged group and 1 is the privileged group\n",
    "S = []\n",
    "for i in Y:\n",
    "    if i == 0:\n",
    "        S.append(bernoulli.rvs(0.35))\n",
    "    else:\n",
    "        S.append(bernoulli.rvs(0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section is to define non-sensitive features for prediction. In non-sensitive features, we need to determine the correlation between the non-sensitive feature and outcome, sensitive attibute and other non-sensitive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['y'] = Y\n",
    "df['sens'] = S\n",
    "df['tmp'] = 1\n",
    "df.groupby(['y', 'sens']).count()\n",
    "tmp = []\n",
    "for i, j in zip(df['y'], df['sens']):\n",
    "    if i == 0 and j == 0:\n",
    "        tmp.append(np.random.poisson(lam=6))\n",
    "    if i == 1 and j == 0:\n",
    "        tmp.append(np.random.poisson(lam=3))\n",
    "    if i == 0 and j == 1:\n",
    "        tmp.append(np.random.poisson(lam=4))\n",
    "    if i == 1 and j == 1:\n",
    "        tmp.append(np.random.poisson(lam=2))\n",
    "df['edu_orig'] = tmp\n",
    "tmp = df['edu_orig'].sort_values().to_list()\n",
    "\n",
    "# create 4 equal-width bins to bin continuous data into categorical\n",
    "def cat_edu(x):\n",
    "    if x <= tmp[int(len(tmp) / 4)]:\n",
    "        return 'edu_cat1'\n",
    "    elif x >= tmp[int(3 * len(tmp) / 4)]:\n",
    "        return 'edu_cat4'\n",
    "    elif x >= tmp[int(2 * len(tmp) / 4)]:\n",
    "        return 'edu_cat3'\n",
    "    else:\n",
    "        return 'edu_cat2'\n",
    "\n",
    "df['edu'] = df['edu_orig'].apply(lambda x: cat_edu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines the first non-sensitive feature, education and how to convert the feature into categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "tmp_list = [(i * 2 - 1) * 0.1 + (j * 2 - 1) * 0.4 + np.log(z + 1) for i, j,\n",
    "            z in zip(df['sens'].tolist(), df['y'].tolist(), df['edu_orig'].tolist())]\n",
    "for i in tmp_list:\n",
    "    temp.append(\n",
    "        np.random.normal(\n",
    "            loc=i,\n",
    "            scale=1.5) +\n",
    "        np.random.exponential(1) -\n",
    "        1)\n",
    "df['occuption_orig'] = temp\n",
    "tmp = temp\n",
    "\n",
    "\n",
    "def cat_occ(x):\n",
    "    if x <= tmp[int(len(tmp) / 4)]:\n",
    "        return 'occ_cat1'\n",
    "    elif x >= tmp[int(3 * len(tmp) / 4)]:\n",
    "        return 'occ_cat4'\n",
    "    elif x >= tmp[int(2 * len(tmp) / 4)]:\n",
    "        return 'occ_cat3'\n",
    "    else:\n",
    "        return 'occ_cat2'\n",
    "\n",
    "\n",
    "df['occuption'] = df['occuption_orig'].apply(lambda x: cat_occ(x))\n",
    "\n",
    "\n",
    "temp = []\n",
    "tmp_list = [(i * 2 - 1) * 0.05 + (j * 2 -1)*0.05 +np.log(z +1) /2 +\n",
    "            np.log(np.abs(k)+1)/5 for i,j,z,k in zip(df['sens'].tolist(),\n",
    "                                                     df['y'].tolist(),\n",
    "                                                     df['edu_orig'].tolist(),\n",
    "                                                     df['occuption_orig'].tolist())]\n",
    "for i in tmp_list:\n",
    "    temp.append(np.random.normal(loc=i,scale=1.5) + np.random.exponential(1)-1)\n",
    "df['age_orig'] = temp\n",
    "tmp = temp\n",
    "def cat_age(x):\n",
    "    if x <= tmp[int(len(tmp) / 4)]:\n",
    "        return 'age_cat1'\n",
    "    elif x >= tmp[int(3 * len(tmp) / 4)]:\n",
    "        return 'age_cat4'\n",
    "    elif x >= tmp[int(2 * len(tmp) / 4)]:\n",
    "        return 'age_cat3'\n",
    "    else:\n",
    "        return 'age_cat2'\n",
    "\n",
    "\n",
    "df['age'] = df['age_orig'].apply(lambda x: cat_age(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the part above, we define the other two non-sensitive features. Now, we have finished the synthetic data construction. <br>\n",
    "Next, we will use the synthetic data to do our missing value experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first define the distort score metrics to use the categorical fix\n",
    "def custom_distort(vold, vnew):\n",
    "    distort = {}\n",
    "    distort['edu'] = pd.DataFrame(\n",
    "        {'edu_cat1': [0., 1., 2., 3., 100.],\n",
    "         'edu_cat2': [1., 0., 1., 2., 100.],\n",
    "         'edu_cat3': [2., 1., 0., 1., 100.],\n",
    "         'edu_cat4': [3., 2., 1., 0., 100.],\n",
    "         'missing': [0., 0., 0., 0., 1.]},\n",
    "        index=['edu_cat1', 'edu_cat2', 'edu_cat3', 'edu_cat4', 'missing'])\n",
    "    distort['occuption'] = pd.DataFrame(\n",
    "        {'occ_cat1': [0., 1., 2., 3.],\n",
    "         'occ_cat2': [1., 0., 1., 2.],\n",
    "         'occ_cat3': [2., 1., 0., 1.],\n",
    "         'occ_cat4': [3., 2., 1., 0.]},\n",
    "        index=['occ_cat1', 'occ_cat2', 'occ_cat3', 'occ_cat4'])\n",
    "    distort['age'] = pd.DataFrame(\n",
    "        {'age_cat1': [0., 1., 2., 3.],\n",
    "         'age_cat2': [1., 0., 1., 2.],\n",
    "         'age_cat3': [2., 1., 0., 1.],\n",
    "         'age_cat4': [3., 2., 1., 0.]},\n",
    "        index=['age_cat1', 'age_cat2', 'age_cat3', 'age_cat4'])\n",
    "    distort['sens'] = pd.DataFrame(\n",
    "        {0.0: [0., 2.],\n",
    "         1.0: [2., 0.]},\n",
    "        index=[0.0, 1.0])\n",
    "    distort['y'] = pd.DataFrame(\n",
    "        {0.0: [0., 2.],\n",
    "         1.0: [2., 0.]},\n",
    "        index=[0.0, 1.0])\n",
    "\n",
    "    total_cost = 0.0\n",
    "    for k in vold:\n",
    "        if k in vnew:\n",
    "            total_cost += distort[k].loc[vnew[k], vold[k]]\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "class CustomDataset(StandardDataset):\n",
    "    \"\"\"Adult Census Income Dataset.\n",
    "\n",
    "    See :file:`aif360/data/raw/adult/README.md`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_name='y',\n",
    "                 favorable_classes=['1'],\n",
    "                 protected_attribute_names=['x_control'],\n",
    "                 privileged_classes=['0'],\n",
    "                 instance_weights_name=None,\n",
    "                 categorical_features=[],\n",
    "                 features_to_keep=[], features_to_drop=[],\n",
    "                 na_values=[''], custom_preprocessing=None,\n",
    "                 df=None,\n",
    "                 metadata=None):\n",
    "\n",
    "        super().__init__(\n",
    "            df=df,\n",
    "            label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop,\n",
    "            na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing,\n",
    "            metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y  sens  tmp  edu_orig       edu  occuption_orig occuption  age_orig  \\\n",
      "0      0     1    1         5  edu_cat4        0.880949  occ_cat1  2.367865   \n",
      "1      0     0    1         6  edu_cat4        3.012828  occ_cat4 -0.077540   \n",
      "2      0     0    1         4  edu_cat3        1.810059  occ_cat4 -1.949661   \n",
      "3      0     1    1         3  edu_cat3        2.441403  occ_cat4 -0.745622   \n",
      "4      0     0    1         2  edu_cat1        0.141826  occ_cat1 -0.425448   \n",
      "...   ..   ...  ...       ...       ...             ...       ...       ...   \n",
      "19995  1     1    1         3  edu_cat3       -0.707350  occ_cat1  0.056468   \n",
      "19996  1     0    1         3  edu_cat3        1.771835  occ_cat4 -0.078363   \n",
      "19997  1     0    1         4  edu_cat3        3.687200  occ_cat4  1.844894   \n",
      "19998  1     0    1         4  edu_cat3        1.045248  occ_cat1 -0.908072   \n",
      "19999  1     0    1         3  edu_cat3        0.706447  occ_cat1  2.685175   \n",
      "\n",
      "            age  \n",
      "0      age_cat4  \n",
      "1      age_cat1  \n",
      "2      age_cat1  \n",
      "3      age_cat1  \n",
      "4      age_cat1  \n",
      "...         ...  \n",
      "19995  age_cat1  \n",
      "19996  age_cat1  \n",
      "19997  age_cat4  \n",
      "19998  age_cat1  \n",
      "19999  age_cat4  \n",
      "\n",
      "[20000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below is how we create missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values\n",
      "3970\n",
      "Total number of observations\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "df['y'] = df['y'].astype(int)\n",
    "df1 = df[['y', 'edu', 'occuption', 'age', 'sens']]\n",
    "tot = []\n",
    "for index, row in df1.iterrows():\n",
    "    result = ''\n",
    "    for j in df1.columns:\n",
    "        result = result + str(row[j])\n",
    "    tot.append(result)\n",
    "df['tmp_feature'] = tot\n",
    "df['mis_prob'] = 0\n",
    "# here, the first element in the column 'tmp_feature' is the label (either 0 or 1 with 0 being negative outcome) \n",
    "# and the last element is sensitive value (either 0 or 1, with 0 being unprivileged)\n",
    "# we define the proportion of missing values in the data. Here the missing values are under MAR\n",
    "for i in df['tmp_feature'].unique():\n",
    "    if i[0] == '1' and i[-1] == '0':\n",
    "        df.loc[df['tmp_feature'] == i, 'mis_prob'] = 0.05\n",
    "    elif i[-1] == '0':\n",
    "        df.loc[df['tmp_feature'] == i, 'mis_prob'] = 0.5\n",
    "    elif i[-1] != '0' and i[0] == '1':\n",
    "        df.loc[df['tmp_feature'] == i, 'mis_prob'] = 0.05\n",
    "    else:\n",
    "        df.loc[df['tmp_feature'] == i, 'mis_prob'] = 0.03\n",
    "new_label = []\n",
    "for i, j in zip(df['mis_prob'], df['edu']):\n",
    "    if np.random.binomial(1, i, 1)[0] == 1:\n",
    "        new_label.append(-1)\n",
    "    else:\n",
    "        new_label.append(j)\n",
    "df['edu'] = new_label\n",
    "print('Total number of missing values')\n",
    "print(len(df.loc[df['edu'] == -1, :].index))\n",
    "print('Total number of observations')\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create our training and test set for training classifier and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_edu(x):\n",
    "    if x == -1:\n",
    "        return 'missing'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "df['edu'] = df['edu'].apply(lambda x: mod_edu(x))\n",
    "\n",
    "\n",
    "df_pos = df.loc[df['y'] == 1, :]\n",
    "df_neg = df.loc[df['y'] == 0, :]\n",
    "\n",
    "df_train_pos, df_test_pos = train_test_split(\n",
    "    df_pos, test_size=1000, random_state=10)\n",
    "df_train_neg, df_test_neg = train_test_split(\n",
    "    df_neg, test_size=1000, random_state=10)\n",
    "df_test = df_test_pos.append(df_test_neg)\n",
    "\n",
    "df_train_tot = df_train_pos.append(df_train_neg)\n",
    "\n",
    "_, df_train_tot_pos = train_test_split(\n",
    "    df_train_pos, test_size=4000, random_state=10)\n",
    "_, df_train_tot_neg = train_test_split(\n",
    "    df_train_neg, test_size=4000, random_state=10)\n",
    "df_train = df_train_tot_pos.append(df_train_tot_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the categorical fix using the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Preprocessing: Objective converged to 0.161315\n"
     ]
    }
   ],
   "source": [
    "orig_cat_train = df_train[['y','age', 'sens', 'occuption', 'edu']]\n",
    "orig_cat_test = df_test[['y', 'age','sens', 'occuption', 'edu']]\n",
    "\n",
    "all_protected_attribute_maps = {\"sens\": {0.0: 0, 1.0: 1}}\n",
    "D_features = ['sens']\n",
    "dataset_orig_cat_train = CustomDataset(\n",
    "    favorable_classes=[1], protected_attribute_names=['sens'], privileged_classes=[\n",
    "        [1]], categorical_features=[\n",
    "            'occuption', 'edu','age'], features_to_keep=[\n",
    "                'occuption', 'edu', 'y', 'sens','age'], df=orig_cat_train, metadata={\n",
    "                    'label_maps': [{1.0: 1, 0.0: 0}], 'protected_attribute_maps': [\n",
    "                                all_protected_attribute_maps[x] for x in D_features]})\n",
    "\n",
    "dataset_orig_cat_test = CustomDataset(\n",
    "    favorable_classes=[1], protected_attribute_names=['sens'], privileged_classes=[\n",
    "        [1]], categorical_features=[\n",
    "            'occuption', 'edu','age'], features_to_keep=[\n",
    "                'occuption', 'edu', 'y', 'sens','age'], df=orig_cat_test, metadata={\n",
    "                    'label_maps': [{1.0: 1, 0.0: 0}], 'protected_attribute_maps': [\n",
    "                                all_protected_attribute_maps[x] for x in D_features]})\n",
    "\n",
    "\n",
    "privileged_groups = [{'sens': 1}]\n",
    "unprivileged_groups = [{'sens': 0}]\n",
    "optim_options = {\n",
    "    \"distortion_fun\": custom_distort,\n",
    "    \"epsilon\": 0.08,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.2, 0.1, 0]\n",
    "}\n",
    "\n",
    "\n",
    "OP = OptimPreproc(OptTools, optim_options,\n",
    "                  unprivileged_groups=unprivileged_groups,\n",
    "                  privileged_groups=privileged_groups)\n",
    "\n",
    "OP = OP.fit(dataset_orig_cat_train)\n",
    "\n",
    "\n",
    "dataset_transf_cat_test = OP.transform(dataset_orig_cat_test, transform_Y=True)\n",
    "dataset_transf_cat_test = dataset_orig_cat_test.align_datasets(\n",
    "    dataset_transf_cat_test)\n",
    "\n",
    "\n",
    "dataset_transf_cat_train = OP.transform(\n",
    "    dataset_orig_cat_train, transform_Y=True)\n",
    "dataset_transf_cat_train = dataset_orig_cat_train.align_datasets(\n",
    "    dataset_transf_cat_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
