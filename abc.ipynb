{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhossai3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "import ot\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "from utils import *\n",
    "from SoftImpute import softimpute, cv_softimpute\n",
    "#from DataSetLoader import dataset_loader, ground_truth\n",
    "from SinkhornImputation import SinkhornImputation\n",
    "from Sinkhorn_CMI import SinkhornImputation_CMI\n",
    "from RR_imputer import RRimputer\n",
    "import matplotlib.pyplot as plt\n",
    "from CMI import *\n",
    "\n",
    "from Inject_Missing_Values import *\n",
    "from Experiment import *\n",
    "\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"test\")\n",
    "import pandas as pd\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "Visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d =  {\n",
    "    \"A\": [10,10,20,40,70,40,50,80,10,40,20,60,50,30,20],\n",
    "    \"B\": [1,1,2,4,7,4,5,8,1,4,2,6,5,3,2],\n",
    "    \"C\":[15,15,30,60,105,60,75,120,15,60,30,90,75,45,30],\n",
    "    \"D\":[45,30,60,32,58,63,25,55,46,78,92,60,34,27,35]\n",
    "\n",
    "}\n",
    "data = pd.DataFrame(d)\n",
    "X= data.iloc[:, :-1]  # Selects all rows and all columns except the last one\n",
    "Y = data.iloc[:, -1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A  B    C\n",
      "0   10  1   15\n",
      "1   10  1   15\n",
      "2   20  2   30\n",
      "3   40  4   60\n",
      "4   70  7  105\n",
      "5   40  4   60\n",
      "6   50  5   75\n",
      "7   80  8  120\n",
      "8   10  1   15\n",
      "9   40  4   60\n",
      "10  20  2   30\n",
      "11  60  6   90\n",
      "12  50  5   75\n",
      "13  30  3   45\n",
      "14  20  2   30\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Percentage MCAR: 24.44%\n",
      "Coloumn Wise Missing Percentage:  A    46.666667\n",
      "B    13.333333\n",
      "C    13.333333\n",
      "D     0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "miss_mcar, index_mcar = Generate_Missing_Value(X,Y,\"MCAR\",25,dependencies=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Percentage MAR25: 6.67%\n",
      "Coloumn Wise Missing Percentage:  A    20.000000\n",
      "B     6.666667\n",
      "C     0.000000\n",
      "D     0.000000\n",
      "dtype: float64\n",
      "       A    B    C   D\n",
      "0   10.0  NaN   15  45\n",
      "1   10.0  1.0   15  30\n",
      "2   20.0  2.0   30  60\n",
      "3   40.0  4.0   60  32\n",
      "4   70.0  7.0  105  58\n",
      "5   40.0  4.0   60  63\n",
      "6    NaN  5.0   75  25\n",
      "7   80.0  8.0  120  55\n",
      "8   10.0  1.0   15  46\n",
      "9   40.0  4.0   60  78\n",
      "10   NaN  2.0   30  92\n",
      "11  60.0  6.0   90  60\n",
      "12   NaN  5.0   75  34\n",
      "13  30.0  3.0   45  27\n",
      "14  20.0  2.0   30  35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dependencies_MAR = {\n",
    "    \"A\": {\n",
    "        \"influencers\": [\"C\",\"B\"],\n",
    "        \"condition\": lambda row: row[\"C\"]+row[\"B\"] > 30  # Just a boolean condition\n",
    "          # Separate probability function\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"influencers\": [\"C\"],\n",
    "        \"condition\": lambda row: True,\n",
    "        \"probability\": lambda row: 0.2 if row[\"C\"] < 30 else 0.1\n",
    "    }\n",
    "}\n",
    "miss_mar, index_mar = Generate_Missing_Value(X,Y,\"MAR\",10,dependencies_MAR)\n",
    "print(miss_mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Percentage MNAR: 6.67%\n",
      "Coloumn Wise Missing Percentage:  A    20.000000\n",
      "B     0.000000\n",
      "C     6.666667\n",
      "D     0.000000\n",
      "dtype: float64\n",
      "       A  B      C   D\n",
      "0   10.0  1   15.0  45\n",
      "1   10.0  1   15.0  30\n",
      "2    NaN  2   30.0  60\n",
      "3   40.0  4   60.0  32\n",
      "4   70.0  7  105.0  58\n",
      "5   40.0  4   60.0  63\n",
      "6   50.0  5   75.0  25\n",
      "7   80.0  8  120.0  55\n",
      "8   10.0  1   15.0  46\n",
      "9    NaN  4    NaN  78\n",
      "10  20.0  2   30.0  92\n",
      "11  60.0  6   90.0  60\n",
      "12   NaN  5   75.0  34\n",
      "13  30.0  3   45.0  27\n",
      "14  20.0  2   30.0  35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dependencies_MNAR = {\n",
    "    \"A\": {\n",
    "        \"condition\": lambda row: True,\n",
    "        \"probability\": lambda row: 0.3 if row[\"A\"] < 30 else 0.1  # High earners are more likely to not report salary\n",
    "    },\n",
    "    \"C\": {\n",
    "        \"condition\": lambda row: row[\"C\"] > 50, # High BP patients may avoid reporting BP readings\n",
    "        \"probability\": lambda row: 0.2 \n",
    "    }\n",
    "    \n",
    "}\n",
    "miss_mnar, index_mnar = Generate_Missing_Value(X,Y,\"MNAR\",10,dependencies_MNAR)\n",
    "print(miss_mnar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CMI:\n",
    "    @staticmethod\n",
    "    def soft_bucketize(data, bucket_specs):\n",
    "        \"\"\"\n",
    "        Softly bucketizes numerical columns using differentiable binning.\n",
    "        \"\"\"\n",
    "        bucketized_data = data.clone()  \n",
    "\n",
    "        for col, bins in bucket_specs.items():\n",
    "            feature = bucketized_data[:, col]  # Select column\n",
    "            min_val, max_val = feature.min(), feature.max()\n",
    "            bin_centers = torch.linspace(min_val, max_val, bins, device=data.device)\n",
    "\n",
    "            # Compute soft bin assignments using Gaussian kernel\n",
    "            distances = torch.abs(feature.unsqueeze(1) - bin_centers.unsqueeze(0))\n",
    "            sigma = (max_val - min_val) / bins  # Bandwidth\n",
    "            soft_assignments = torch.exp(-0.5 * (distances / sigma) ** 2)\n",
    "            soft_assignments = soft_assignments / soft_assignments.sum(dim=1, keepdim=True)\n",
    "\n",
    "            # Compute differentiable bin values\n",
    "            new_feature = (soft_assignments @ bin_centers).squeeze()\n",
    "\n",
    "            \n",
    "            bucketized_data = torch.cat([\n",
    "            bucketized_data[:, :col], \n",
    "            new_feature.unsqueeze(1),  \n",
    "            bucketized_data[:, col+1:]\n",
    "        ], dim=1)  \n",
    "\n",
    "        return bucketized_data \n",
    "    @staticmethod\n",
    "    def compute_probabilities_torch(data, bins=10):\n",
    "        \"\"\"Differentiable soft binning using Gaussian kernel smoothing.\"\"\"\n",
    "        min_val, max_val = data.min(), data.max()\n",
    "        bin_centers = torch.linspace(min_val, max_val, bins, device=data.device)\n",
    "\n",
    "        # Compute soft binning with Gaussian kernel\n",
    "        distances = torch.abs(data.unsqueeze(1) - bin_centers.unsqueeze(0))\n",
    "        sigma = (max_val - min_val) / bins\n",
    "        soft_assignments = torch.exp(-0.5 * (distances / sigma) ** 2)\n",
    "        soft_assignments /= soft_assignments.sum(dim=1, keepdim=True)\n",
    "\n",
    "        return soft_assignments.mean(dim=0)  # Normalize\n",
    "\n",
    "    @staticmethod\n",
    "    def conditional_mutual_information(data, X_cols, Y_cols, Z_cols, bucket_specs):\n",
    "        \"\"\"\n",
    "        Compute fully differentiable Conditional Mutual Information (CMI).\n",
    "        \"\"\"\n",
    "        if not data.requires_grad:\n",
    "            raise ValueError(\"Input data must have requires_grad=True.\")\n",
    "\n",
    "        # Bucketize data\n",
    "        bucketized_data = CMI.soft_bucketize(data, bucket_specs)\n",
    "\n",
    "        # Compute probability distributions\n",
    "        prob_cache = {}\n",
    "        columns_list = [Z_cols, X_cols + Z_cols, Y_cols + Z_cols, X_cols + Y_cols + Z_cols]\n",
    "        keys = ['Z', 'XZ', 'YZ', 'XYZ']\n",
    "\n",
    "        for key, cols in zip(keys, columns_list):\n",
    "            prob_cache[key] = CMI.compute_probabilities_torch(bucketized_data, cols)\n",
    "\n",
    "        # Retrieve precomputed probabilities\n",
    "        P_Z = prob_cache['Z']\n",
    "        P_XZ = prob_cache['XZ']\n",
    "        P_YZ = prob_cache['YZ']\n",
    "        P_XYZ = prob_cache['XYZ']\n",
    "\n",
    "        print(\"P_XYZ shape:\", P_XYZ.shape)\n",
    "        print(\"P_XZ shape:\", P_XZ.shape)\n",
    "        print(\"P_YZ shape:\", P_YZ.shape)\n",
    "        print(\"P_Z shape:\", P_Z.shape)\n",
    "\n",
    "        # Ensure matching probability shapes\n",
    "        min_size = min(P_XYZ.shape[1], P_XZ.shape[1], P_YZ.shape[1], P_Z.shape[1])\n",
    "        P_XYZ, P_XZ, P_YZ, P_Z = P_XYZ[:, :min_size], P_XZ[:, :min_size], P_YZ[:, :min_size], P_Z[:, :min_size]\n",
    "\n",
    "        # Compute safe log ratio for stability\n",
    "        safe_ratio = ((P_XYZ * P_Z) / (P_XZ * P_YZ + 1e-10)).clamp(min=1.0)  # ðŸ”¥ Fix: Ensure ratio is â‰¥ 1\n",
    "\n",
    "    \n",
    "        cmi_values = P_XYZ * torch.log2(safe_ratio)\n",
    "        cmi = torch.sum(cmi_values)\n",
    "\n",
    "    \n",
    "        cmi = torch.clamp(cmi, min=0.0)\n",
    "\n",
    "        return cmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file into a Pandas DataFrame\n",
    "groundTruth = pd.read_csv(r'C:\\Users\\zhossai3\\Desktop\\Fair_Imputation\\Data\\Diabetic_Ground_Truth.csv', delimiter=',', header=0)\n",
    "\n",
    "# Store feature columns in a DataFrame\n",
    "\n",
    "\n",
    "X= groundTruth.iloc[:, :-1]  # Selects all rows and all columns except the last one\n",
    "Y = groundTruth.iloc[:, -1]  # Selects all rows and only the last column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth_tensor = torch.tensor(scale(groundTruth)) #converting groundTruth to Tensor, z-score scaling\n",
    "groundTruth_tensor.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_specs = {\n",
    "     \n",
    "    0: 4,   # Column 0 â†’ age (4 bins)\n",
    "    1: 2,  # Column 1 â†’ gender (2 bins)\n",
    "    17: 2,  # Column 17 â†’ label  (2 bins)\n",
    "       # Column 2 â†’ Family Diabetic (2 bins)\n",
    "       # Column 3 â†’ HighBP (2 bins)\n",
    "    4: 4, #column 4 ->  PhysicallyActive (4 bins)\n",
    "    5: 20 #column 5-> BMI (20 bins)\n",
    "    \n",
    "}\n",
    "\n",
    "# Define multiple attributes for X, Y, Z\n",
    "X_cols = [0,1]  # Bucketized sensitive attributes (e.g., sex, race, age)\n",
    "Y_cols = [17]     # Bucketized outcome-related attributes\n",
    "Z_cols = [3,2,4,5,6,7]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= CMI.soft_bucketize(groundTruth_tensor,bucket_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1254,  7.9776], grad_fn=<Unique2Backward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.unique(d[:,15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linspace() received an invalid combination of arguments - got (Tensor, Tensor, list, device=torch.device), but expected one of:\n * (Tensor start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Tensor start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute Conditional Mutual Information (CMI)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cmi_value \u001b[38;5;241m=\u001b[39m \u001b[43mCMI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditional_mutual_information\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroundTruth_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_specs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCMI Estimate:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cmi_value)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCMI grad_fn:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cmi_value\u001b[38;5;241m.\u001b[39mgrad_fn)  \u001b[38;5;66;03m# Should NOT be None\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[47], line 65\u001b[0m, in \u001b[0;36mCMI.conditional_mutual_information\u001b[1;34m(data, X_cols, Y_cols, Z_cols, bucket_specs)\u001b[0m\n\u001b[0;32m     62\u001b[0m keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXZ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYZ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXYZ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, cols \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, columns_list):\n\u001b[1;32m---> 65\u001b[0m     prob_cache[key] \u001b[38;5;241m=\u001b[39m \u001b[43mCMI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_probabilities_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucketized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Retrieve precomputed probabilities\u001b[39;00m\n\u001b[0;32m     68\u001b[0m P_Z \u001b[38;5;241m=\u001b[39m prob_cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[47], line 38\u001b[0m, in \u001b[0;36mCMI.compute_probabilities_torch\u001b[1;34m(data, bins)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Differentiable soft binning using Gaussian kernel smoothing.\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m min_val, max_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmin(), data\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m---> 38\u001b[0m bin_centers \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Compute soft binning with Gaussian kernel\u001b[39;00m\n\u001b[0;32m     41\u001b[0m distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(data\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m bin_centers\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: linspace() received an invalid combination of arguments - got (Tensor, Tensor, list, device=torch.device), but expected one of:\n * (Tensor start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Tensor start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute Conditional Mutual Information (CMI)\n",
    "cmi_value = CMI.conditional_mutual_information(groundTruth_tensor, X_cols, Y_cols, Z_cols, bucket_specs)\n",
    "\n",
    "\n",
    "print(\"CMI Estimate:\", cmi_value)\n",
    "print(\"CMI grad_fn:\", cmi_value.grad_fn)  # Should NOT be None\n",
    "\n",
    "\n",
    "cmi_value.backward()\n",
    "\n",
    "\n",
    "#print(\"Gradient of data:\", data.grad)  # Should NOT be None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Conditional Mutual Information I(X;Y | Z): 0.0542\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def estimate_entropy(kde, samples):\n",
    "    \"\"\"Estimate differential entropy using KDE\"\"\"\n",
    "    log_prob = kde.score_samples(samples)\n",
    "    return -np.mean(log_prob)\n",
    "\n",
    "def conditional_mutual_information(X, Y, Z, bandwidth=0.2):\n",
    "    \"\"\"Estimate Conditional Mutual Information I(X;Y | Z) using KDE\"\"\"\n",
    "    \n",
    "    # Stack variables for joint distributions\n",
    "    XZ = np.vstack([X, Z]).T\n",
    "    YZ = np.vstack([Y, Z]).T\n",
    "    XYZ = np.vstack([X, Y, Z]).T\n",
    "    Z_reshaped = Z.reshape(-1, 1)\n",
    "    \n",
    "    # Fit KDE models\n",
    "    kde_xz = KernelDensity(bandwidth=bandwidth).fit(XZ)\n",
    "    kde_yz = KernelDensity(bandwidth=bandwidth).fit(YZ)\n",
    "    kde_xyz = KernelDensity(bandwidth=bandwidth).fit(XYZ)\n",
    "    kde_z = KernelDensity(bandwidth=bandwidth).fit(Z_reshaped)\n",
    "\n",
    "    # Compute entropies\n",
    "    h_x_given_z = estimate_entropy(kde_xz, XZ) - estimate_entropy(kde_z, Z_reshaped)\n",
    "    h_y_given_z = estimate_entropy(kde_yz, YZ) - estimate_entropy(kde_z, Z_reshaped)\n",
    "    h_xy_given_z = estimate_entropy(kde_xyz, XYZ) - estimate_entropy(kde_z, Z_reshaped)\n",
    "\n",
    "    # Compute Conditional Mutual Information\n",
    "    cmi = h_x_given_z + h_y_given_z - h_xy_given_z\n",
    "    return cmi\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "Z = np.random.uniform(-1, 1, 1000)  # Conditioning variable\n",
    "X = Z + np.random.normal(0, 0.5, 1000)  # X depends on Z\n",
    "Y = Z + np.random.normal(0, 0.5, 1000)  # Y depends on Z\n",
    "\n",
    "# Compute Conditional Mutual Information\n",
    "cmi_value = conditional_mutual_information(X, Y, Z)\n",
    "print(f\"Estimated Conditional Mutual Information I(X;Y | Z): {cmi_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Conditional Mutual Information I(X; Y | Z): 0.0997\n",
      "Gradient of CMI wrt Z: tensor([ 0.0020,  0.0014, -0.0006, -0.0027, -0.0026])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=0.2):\n",
    "    \"\"\"Computes Gaussian kernel similarity\"\"\"\n",
    "    diff = x.unsqueeze(1) - y.unsqueeze(0)\n",
    "    return torch.exp(-torch.sum(diff**2, dim=-1) / (2 * sigma**2))\n",
    "\n",
    "def kde_entropy(samples, sigma=0.2):\n",
    "    \"\"\"Differentiable KDE-based entropy estimation\"\"\"\n",
    "    n = samples.shape[0]\n",
    "    kernel_matrix = gaussian_kernel(samples, samples, sigma)\n",
    "    density_estimates = kernel_matrix.mean(dim=1)\n",
    "    entropy = -torch.mean(torch.log(density_estimates + 1e-10))  # Prevent log(0)\n",
    "    return entropy\n",
    "\n",
    "def conditional_mutual_information_kde(X, Y, Z, sigma=0.2):\n",
    "    \"\"\"Computes I(X; Y | Z) using differentiable KDE\"\"\"\n",
    "    h_x_given_z = kde_entropy(torch.cat([X, Z], dim=1), sigma) - kde_entropy(Z, sigma)\n",
    "    h_y_given_z = kde_entropy(torch.cat([Y, Z], dim=1), sigma) - kde_entropy(Z, sigma)\n",
    "    h_xy_given_z = kde_entropy(torch.cat([X, Y, Z], dim=1), sigma) - kde_entropy(Z, sigma)\n",
    "    return h_x_given_z + h_y_given_z - h_xy_given_z\n",
    "\n",
    "# Generate synthetic data (same as before)\n",
    "N = 1000\n",
    "Z = torch.randn(N, 1, requires_grad=True)  # Conditioning variable\n",
    "X = Z + 0.5 * torch.randn(N, 1, requires_grad=True)  # X depends on Z\n",
    "Y = Z + 0.5 * torch.randn(N, 1, requires_grad=True)  # Y depends on Z\n",
    "\n",
    "# Compute CMI with differentiable KDE\n",
    "cmi_value = conditional_mutual_information_kde(X, Y, Z)\n",
    "print(f\"Estimated Conditional Mutual Information I(X; Y | Z): {cmi_value.item():.4f}\")\n",
    "\n",
    "# Compute gradient with respect to Z\n",
    "cmi_value.backward()\n",
    "print(f\"Gradient of CMI wrt Z: {Z.grad[:5].flatten()}\")  # Check gradient values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Conditional Mutual Information: 1.4053\n",
      "Gradient of CMI wrt Z: tensor([ 0.0043,  0.0036,  0.0011, -0.0012, -0.0028,  0.0042, -0.0004,  0.0012,\n",
      "        -0.0077,  0.0005])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=0.2):\n",
    "    \"\"\"Computes Gaussian kernel similarity\"\"\"\n",
    "    diff = x.unsqueeze(1) - y.unsqueeze(0)  # Pairwise differences\n",
    "    return torch.exp(-torch.sum(diff**2, dim=-1) / (2 * sigma**2))  # Gaussian Kernel\n",
    "\n",
    "def kde_entropy(samples, sigma=0.2):\n",
    "    \"\"\"Differentiable Kernel Density Estimation (KDE) for entropy estimation\"\"\"\n",
    "    kernel_matrix = gaussian_kernel(samples, samples, sigma)\n",
    "    density_estimates = kernel_matrix.mean(dim=1)\n",
    "    entropy = -torch.mean(torch.log(density_estimates + 1e-10))  # Prevent log(0)\n",
    "    return entropy\n",
    "\n",
    "def conditional_differential_entropy(X, Z, sigma=0.2):\n",
    "    \"\"\"Computes conditional differential entropy h(X|Z) = h(X,Z) - h(Z)\"\"\"\n",
    "    XZ = torch.cat([X, Z], dim=1)  # Concatenating X and Z\n",
    "    h_xz = kde_entropy(XZ, sigma)\n",
    "    h_z = kde_entropy(Z, sigma)\n",
    "    return h_xz - h_z  # Conditional entropy h(X|Z)\n",
    "\n",
    "def conditional_mutual_information(X, Y, Z, sigma=0.2):\n",
    "    \"\"\"Computes differentiable Conditional Mutual Information I(X; Y | Z)\"\"\"\n",
    "    h_x_given_z = conditional_differential_entropy(X, Z, sigma)\n",
    "    h_y_given_z = conditional_differential_entropy(Y, Z, sigma)\n",
    "    h_xy_given_z = conditional_differential_entropy(torch.cat([X, Y], dim=1), Z, sigma)\n",
    "\n",
    "    # Compute CMI\n",
    "    cmi = h_x_given_z + h_y_given_z - h_xy_given_z\n",
    "    return cmi\n",
    "\n",
    "# Generate Multi-Dimensional Continuous Data (With AutoGrad Enabled)\n",
    "torch.manual_seed(42)\n",
    "N = 1000  # Number of samples\n",
    "dim_X, dim_Y, dim_Z = 2, 2, 2  # Dimensions\n",
    "\n",
    "Z = torch.randn(N, dim_Z, requires_grad=True)  # Conditioning variables\n",
    "X = Z @ torch.randn(dim_Z, dim_X) + 0.5 * torch.randn(N, dim_X, requires_grad=True)  # X depends on Z\n",
    "Y = Z @ torch.randn(dim_Z, dim_Y) + 0.5 * torch.randn(N, dim_Y, requires_grad=True)  # Y depends on Z\n",
    "\n",
    "# Compute Differentiable Conditional Mutual Information\n",
    "cmi_value = conditional_mutual_information(X, Y, Z)\n",
    "print(f\"Estimated Conditional Mutual Information: {cmi_value.item():.4f}\")\n",
    "\n",
    "# Compute Gradient with respect to Z\n",
    "cmi_value.backward()\n",
    "print(f\"Gradient of CMI wrt Z: {Z.grad[:5].flatten()}\")  # Displaying first few gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Estimated Conditional Mutual Information': 0.5821194434625134,\n",
       " 'Gradient of CMI wrt Z (First 5 Values)': [-0.0017520666312357226,\n",
       "  0.0023430754199654973,\n",
       "  -0.0036632118552742997,\n",
       "  -0.00024082595278532992,\n",
       "  0.002346146218324777,\n",
       "  0.0020583589071262974,\n",
       "  0.0014867170125502354,\n",
       "  0.00039828858704201467,\n",
       "  -0.000707830809133528,\n",
       "  0.0020481994992923667,\n",
       "  0.001364668192037918,\n",
       "  0.0017069824657126162,\n",
       "  0.00229573262244321,\n",
       "  0.0004573957341224032,\n",
       "  0.0066925142567881265]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simulated real-world dataset with multiple X, Y, Z features\n",
    "N = 1000  # Number of samples\n",
    "dim_X, dim_Y, dim_Z = 2, 1, 3  # Multi-dimensional features\n",
    "\n",
    "# Generate a synthetic dataset (replace with real-world data)\n",
    "torch.manual_seed(42)\n",
    "Z = torch.randn(N, dim_Z, requires_grad=True)  # Conditioning variables (3D)\n",
    "X = Z @ torch.randn(dim_Z, dim_X) + 0.5 * torch.randn(N, dim_X, requires_grad=True)  # X depends on Z\n",
    "Y = Z @ torch.randn(dim_Z, dim_Y) + 0.5 * torch.randn(N, dim_Y, requires_grad=True)  # Y depends on Z\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=0.2):\n",
    "    \"\"\"Computes Gaussian kernel similarity\"\"\"\n",
    "    diff = x.unsqueeze(1) - y.unsqueeze(0)  # Pairwise differences\n",
    "    return torch.exp(-torch.sum(diff**2, dim=-1) / (2 * sigma**2))  # Gaussian Kernel\n",
    "\n",
    "def kde_entropy(samples, sigma=0.2):\n",
    "    \"\"\"Differentiable KDE-based entropy estimation\"\"\"\n",
    "    kernel_matrix = gaussian_kernel(samples, samples, sigma)\n",
    "    density_estimates = kernel_matrix.mean(dim=1)\n",
    "    entropy = -torch.mean(torch.log(density_estimates + 1e-10))  # Prevent log(0)\n",
    "    return entropy\n",
    "\n",
    "def conditional_differential_entropy(X, Z, sigma=0.2):\n",
    "    \"\"\"Computes conditional differential entropy h(X|Z) = h(X,Z) - h(Z)\"\"\"\n",
    "    XZ = torch.cat([X, Z], dim=1)  # Concatenating X and Z\n",
    "    h_xz = kde_entropy(XZ, sigma)\n",
    "    h_z = kde_entropy(Z, sigma)\n",
    "    return h_xz - h_z  # Conditional entropy h(X|Z)\n",
    "\n",
    "def conditional_mutual_information(X, Y, Z, sigma=0.2):\n",
    "    \"\"\"Computes differentiable Conditional Mutual Information I(X; Y | Z)\"\"\"\n",
    "    h_x_given_z = conditional_differential_entropy(X, Z, sigma)\n",
    "    h_y_given_z = conditional_differential_entropy(Y, Z, sigma)\n",
    "    h_xy_given_z = conditional_differential_entropy(torch.cat([X, Y], dim=1), Z, sigma)\n",
    "\n",
    "    # Compute CMI\n",
    "    cmi = h_x_given_z + h_y_given_z - h_xy_given_z\n",
    "    return cmi\n",
    "\n",
    "# Compute CMI on Multi-Dimensional Data\n",
    "cmi_value = conditional_mutual_information(X, Y, Z)\n",
    "\n",
    "# Compute Gradient with respect to Z\n",
    "cmi_value.backward()\n",
    "\n",
    "# Display results\n",
    "{\n",
    "    \"Estimated Conditional Mutual Information\": cmi_value.item(),\n",
    "    \"Gradient of CMI wrt Z (First 5 Values)\": Z.grad[:5].flatten().tolist()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMI Tensor: 0.3615305423736572, grad_fn: <SubBackward0 object at 0x000001F8A6519030>\n",
      "Estimated Conditional Mutual Information: 0.3615305423736572\n",
      "Gradient of CMI with respect to Z (first 5 values): [-1.862645149230957e-09, 4.6566128730773926e-07, -4.0978193283081055e-08, -0.000372511800378561, 2.7348287403583527e-06, 0.001090371049940586, -1.2484160833992064e-06, -0.0027197087183594704, -1.4062970876693726e-06, 0.0008600093424320221]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def kde_entropy_torch(samples, sigma=0.2):\n",
    "    \"\"\"Estimate differential entropy using a Gaussian Kernel with PyTorch (AutoGrad enabled).\"\"\"\n",
    "    pairwise_distances = torch.cdist(samples, samples, p=2)  # Euclidean distance\n",
    "    kernel_matrix = torch.exp(-pairwise_distances ** 2 / (2 * sigma ** 2))\n",
    "    density_estimates = kernel_matrix.mean(dim=1)\n",
    "    entropy = -torch.mean(torch.log(density_estimates + 1e-10))  # Prevent log(0)\n",
    "    return entropy\n",
    "\n",
    "def conditional_differential_entropy_torch(X, Z, sigma=0.2):\n",
    "    \"\"\"Computes conditional differential entropy h(X|Z) = h(X,Z) - h(Z) with AutoGrad support\"\"\"\n",
    "    XZ = torch.cat([X, Z], dim=1)  # Concatenating X and Z\n",
    "    h_xz = kde_entropy_torch(XZ, sigma)\n",
    "    h_z = kde_entropy_torch(Z, sigma)\n",
    "    return h_xz - h_z  # Conditional entropy h(X|Z)\n",
    "\n",
    "def conditional_mutual_information_torch(X, Y, Z, sigma=0.2):\n",
    "    \"\"\"Computes differentiable Conditional Mutual Information I(X; Y | Z)\"\"\"\n",
    "    h_x_given_z = conditional_differential_entropy_torch(X, Z, sigma)\n",
    "    h_y_given_z = conditional_differential_entropy_torch(Y, Z, sigma)\n",
    "    h_xy_given_z = conditional_differential_entropy_torch(torch.cat([X, Y], dim=1), Z, sigma)\n",
    "\n",
    "    # Compute CMI\n",
    "    cmi = h_x_given_z + h_y_given_z - h_xy_given_z\n",
    "    return cmi\n",
    "\n",
    "def compute_cmi_from_csv_with_grad(file_path, x_cols, y_cols, z_cols):\n",
    "    \"\"\"Loads dataset, selects columns, normalizes, and computes differentiable CMI\"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Select columns\n",
    "    X = df.iloc[:, x_cols].values  # Select X columns\n",
    "    Y = df.iloc[:, y_cols].values  # Select Y columns\n",
    "    Z = df.iloc[:, z_cols].values  # Select Z columns\n",
    "\n",
    "    # Normalize data for stability in entropy estimation\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    Y = scaler.fit_transform(Y)\n",
    "    Z = scaler.fit_transform(Z)\n",
    "\n",
    "    # Convert to PyTorch tensors with requires_grad=True for AutoGrad\n",
    "    X_torch = torch.tensor(X, dtype=torch.float32, requires_grad=True)\n",
    "    Y_torch = torch.tensor(Y, dtype=torch.float32, requires_grad=True)\n",
    "    Z_torch = torch.tensor(Z, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    # Compute CMI (Differentiable)\n",
    "    cmi_value = conditional_mutual_information_torch(X_torch, Y_torch, Z_torch)\n",
    "    print(f\"CMI Tensor: {cmi_value}, grad_fn: {cmi_value.grad_fn}\")\n",
    "    \n",
    "    # Compute Gradient with respect to Z\n",
    "    cmi_value.backward()\n",
    "\n",
    "    return cmi_value, Z_torch.grad\n",
    "\n",
    "# Example Usage\n",
    "file_path = (r'C:\\Users\\zhossai3\\Desktop\\Fair_Imputation\\Data\\Diabetic_Ground_Truth.csv') # Replace with your actual CSV file\n",
    "x_cols = [ 0,1]  # Column indices for X\n",
    "y_cols = [17]  # Column indices for Y\n",
    "z_cols = [4,5]  # Column indices for Z\n",
    "\n",
    "# Compute Conditional Mutual Information\n",
    "cmi_result, z_grad = compute_cmi_from_csv_with_grad(file_path, x_cols, y_cols, z_cols)\n",
    "print(f\"Estimated Conditional Mutual Information: {cmi_result}\")\n",
    "print(f\"Gradient of CMI with respect to Z (first 5 values): {z_grad[:5].flatten().tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth = pd.read_csv(r'C:\\Users\\zhossai3\\Desktop\\Fair_Imputation\\Data\\Diabetic_Ground_Truth.csv', delimiter=',', header=0)\n",
    "groundTruth_tensor = torch.tensor(scale(groundTruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Conditional Mutual Information (Bucketized Data): 0.3567\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def bucketize_tensor(data, bucket_specs):\n",
    "    \"\"\"Bucketizes each column of the PyTorch tensor based on `bucket_specs`.\"\"\"\n",
    "    bucketized_data = data.clone()  \n",
    "\n",
    "    for col, bins in bucket_specs.items():\n",
    "            feature = bucketized_data[:, col]  # Select column\n",
    "            min_val, max_val = feature.min(), feature.max()\n",
    "            bin_centers = torch.linspace(min_val, max_val, bins, device=data.device)\n",
    "\n",
    "            # Compute soft bin assignments using Gaussian kernel\n",
    "            distances = torch.abs(feature.unsqueeze(1) - bin_centers.unsqueeze(0))\n",
    "            sigma = (max_val - min_val) / bins  # Bandwidth\n",
    "            soft_assignments = torch.exp(-0.5 * (distances / sigma) ** 2)\n",
    "            soft_assignments = soft_assignments / soft_assignments.sum(dim=1, keepdim=True)\n",
    "\n",
    "            # Compute differentiable bin values\n",
    "            new_feature = (soft_assignments @ bin_centers).squeeze()\n",
    "\n",
    "            \n",
    "            bucketized_data = torch.cat([\n",
    "            bucketized_data[:, :col], \n",
    "            new_feature.unsqueeze(1),  \n",
    "            bucketized_data[:, col+1:]\n",
    "        ], dim=1)  \n",
    "\n",
    "    return bucketized_data  # Convert back to tensor\n",
    "\n",
    "def kde_entropy_torch(samples, sigma=0.2):\n",
    "    \"\"\"Estimate differential entropy using a Gaussian Kernel with PyTorch (AutoGrad enabled).\"\"\"\n",
    "    pairwise_distances = torch.cdist(samples, samples, p=2)  # Euclidean distance\n",
    "    kernel_matrix = torch.exp(-pairwise_distances ** 2 / (2 * sigma ** 2))\n",
    "    density_estimates = kernel_matrix.mean(dim=1)\n",
    "    entropy = -torch.mean(torch.log(density_estimates + 1e-10))  # Prevent log(0)\n",
    "    return entropy\n",
    "\n",
    "def conditional_differential_entropy_torch(X, Z, sigma=0.2):\n",
    "    \"\"\"Computes conditional differential entropy h(X|Z) = h(X,Z) - h(Z)\"\"\"\n",
    "    XZ = torch.cat([X, Z], dim=1)  # Concatenating X and Z\n",
    "    h_xz = kde_entropy_torch(XZ, sigma)\n",
    "    h_z = kde_entropy_torch(Z, sigma)\n",
    "    return h_xz - h_z  # Conditional entropy h(X|Z)\n",
    "\n",
    "def conditional_mutual_information_torch(X, Y, Z, sigma=0.2):\n",
    "    \"\"\"Computes differentiable Conditional Mutual Information I(X; Y | Z)\"\"\"\n",
    "    h_x_given_z = conditional_differential_entropy_torch(X, Z, sigma)\n",
    "    h_y_given_z = conditional_differential_entropy_torch(Y, Z, sigma)\n",
    "    h_xy_given_z = conditional_differential_entropy_torch(torch.cat([X, Y], dim=1), Z, sigma)\n",
    "\n",
    "    # Compute CMI\n",
    "    cmi = h_x_given_z + h_y_given_z - h_xy_given_z\n",
    "    return cmi\n",
    "\n",
    "def compute_cmi_with_bucketized_tensor(data_tensor, bucket_specs, X_cols, Y_cols, Z_cols):\n",
    "    \"\"\"\n",
    "    Applies bucketization, selects columns, and computes differentiable CMI.\n",
    "    \n",
    "    Args:\n",
    "    - data_tensor: Input PyTorch tensor.\n",
    "    - bucket_specs: Dictionary specifying number of bins per column.\n",
    "    - X_cols, Y_cols, Z_cols: Lists of column indices for X, Y, Z.\n",
    "    \n",
    "    Returns:\n",
    "    - cmi_value: Estimated CMI.\n",
    "    - z_grad: Gradient of CMI w.r.t. Z.\n",
    "    \"\"\"\n",
    "    # Apply bucketization\n",
    "    bucketized_tensor = bucketize_tensor(data_tensor, bucket_specs)\n",
    "\n",
    "    # Select features for X, Y, Z\n",
    "    X_torch = bucketized_tensor[:, X_cols].clone().detach().requires_grad_(True)\n",
    "    Y_torch = bucketized_tensor[:, Y_cols].clone().detach().requires_grad_(True)\n",
    "    Z_torch = bucketized_tensor[:, Z_cols].clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Compute CMI (Differentiable)\n",
    "    cmi_value = conditional_mutual_information_torch(X_torch, Y_torch, Z_torch)\n",
    "\n",
    "    # Compute Gradient with respect to Z\n",
    "   \n",
    "\n",
    "    return cmi_value\n",
    "\n",
    "# Example Input Data (Replace with your real tensor)\n",
    "torch.manual_seed(42)\n",
    "  # Example tensor with 8 features\n",
    "\n",
    "# Bucketization Dictionary\n",
    "bucket_specs = {\n",
    "    0: 4,   # Column 0 â†’ age (4 bins)\n",
    "    1: 2,   # Column 1 â†’ gender (2 bins)\n",
    "    17: 2,  # Column 17 â†’ label (2 bins)\n",
    "    4: 4,   # Column 4 â†’ Physically Active (4 bins)\n",
    "    5: 20   # Column 5 â†’ BMI (20 bins)\n",
    "}\n",
    "\n",
    "# Column Selection\n",
    "X_cols = [0, 1]         # Sensitive attributes\n",
    "Y_cols = [17]           # Outcome-related attributes\n",
    "Z_cols = [4,5,15]  # Additional covariates\n",
    "\n",
    "# Compute CMI with Bucketization\n",
    "cmi_result = compute_cmi_with_bucketized_tensor(groundTruth_tensor, bucket_specs, X_cols, Y_cols, Z_cols)\n",
    "\n",
    "print(f\"Estimated Conditional Mutual Information (Bucketized Data): {cmi_result:.4f}\")\n",
    "#print(f\"Gradient of CMI with respect to Z (first 5 values): {z_grad[:5].flatten().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
