{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pot in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.9.4)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pot) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pot) (1.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geomloss in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geomloss) (2.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geomloss) (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->geomloss) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->geomloss) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->geomloss) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->geomloss) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->geomloss) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->geomloss) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->geomloss) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch->geomloss) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->geomloss) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\zhossai3\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.14.1)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install pot\n",
    "!pip install torch\n",
    "!pip install geomloss\n",
    "!pip install wget\n",
    "!pip install numpy scipy sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhossai3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "import ot\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "from utils import *\n",
    "from SoftImpute import softimpute, cv_softimpute\n",
    "from DataSetLoader import dataset_loader, ground_truth\n",
    "from SinkhornImputation import SinkhornImputation\n",
    "from Sinkhorn_CMI import SinkhornImputation_CMI\n",
    "from RR_imputer import RRimputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\zhossai3\\\\Desktop\\\\My code\\\\OT+ Fairness\\\\Data\\\\Student_performance_data _.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mzhossai3\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMy code\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOT+ Fairness\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mStudent_performance_data _.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhossai3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhossai3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\zhossai3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhossai3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\zhossai3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\zhossai3\\\\Desktop\\\\My code\\\\OT+ Fairness\\\\Data\\\\Student_performance_data _.csv'"
     ]
    }
   ],
   "source": [
    "s = pd.read_csv(r'C:\\Users\\zhossai3\\Desktop\\My code\\OT+ Fairness\\Data\\Student_performance_data _.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age  Gender  Ethnicity  ParentalEducation  StudyTimeWeekly  Absences  \\\n",
      "0      17       1          0                  2        19.833723         7   \n",
      "1      18       0          0                  1        15.408756         0   \n",
      "2      15       0          2                  3         4.210570        26   \n",
      "3      17       1          0                  3        10.028829        14   \n",
      "4      17       1          0                  2         4.672495        17   \n",
      "...   ...     ...        ...                ...              ...       ...   \n",
      "2387   18       1          0                  3        10.680555         2   \n",
      "2388   17       0          0                  1         7.583217         4   \n",
      "2389   16       1          0                  2         6.805500        20   \n",
      "2390   16       1          1                  0        12.416653        17   \n",
      "2391   16       1          0                  2        17.819907        13   \n",
      "\n",
      "      Tutoring  ParentalSupport  Extracurricular  Sports  Music  Volunteering  \\\n",
      "0            1                2                0       0      1             0   \n",
      "1            0                1                0       0      0             0   \n",
      "2            0                2                0       0      0             0   \n",
      "3            0                3                1       0      0             0   \n",
      "4            1                3                0       0      0             0   \n",
      "...        ...              ...              ...     ...    ...           ...   \n",
      "2387         0                4                1       0      0             0   \n",
      "2388         1                4                0       1      0             0   \n",
      "2389         0                2                0       0      0             1   \n",
      "2390         0                2                0       1      1             0   \n",
      "2391         0                2                0       0      0             1   \n",
      "\n",
      "           GPA  GradeClass  \n",
      "0     2.929196           2  \n",
      "1     3.042915           1  \n",
      "2     0.112602           4  \n",
      "3     2.054218           3  \n",
      "4     1.288061           4  \n",
      "...        ...         ...  \n",
      "2387  3.455509           0  \n",
      "2388  3.279150           4  \n",
      "2389  1.142333           2  \n",
      "2390  1.803297           1  \n",
      "2391  2.140014           1  \n",
      "\n",
      "[2392 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = {}\n",
    "with open(r'C:\\Users\\zhossai3\\Desktop\\My code\\OT+ Fairness\\Data\\Student_performance_data _.csv', 'rb') as f:\n",
    "        df = pd.read_csv(f, delimiter=',', header = 0)\n",
    "        Xy['data'] = df.values[:, :-1]\n",
    "        Xy['target'] =  df.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "ground_truth_numpy = (Xy['data'])\n",
    "print(type(ground_truth_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.          1.          0.         ...  1.          0.\n",
      "   2.92919559]\n",
      " [18.          0.          0.         ...  0.          0.\n",
      "   3.04291483]\n",
      " [15.          0.          2.         ...  0.          0.\n",
      "   0.11260225]\n",
      " ...\n",
      " [16.          1.          0.         ...  0.          1.\n",
      "   1.14233288]\n",
      " [16.          1.          1.         ...  1.          0.\n",
      "   1.80329676]\n",
      " [16.          1.          0.         ...  0.          1.\n",
      "   2.14001388]]\n"
     ]
    }
   ],
   "source": [
    "print(Xy['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0\n",
      "0.0\n",
      "3.7913301488105686\n",
      "1.0\n",
      "6.288757915673482\n"
     ]
    }
   ],
   "source": [
    "print(np.max(ground_truth_numpy))\n",
    "print(np.min(ground_truth_numpy))\n",
    "print(np.mean(ground_truth_numpy))\n",
    "print(np.median(ground_truth_numpy))\n",
    "print(np.std(ground_truth_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.          1.          0.         ...  1.          0.\n",
      "   2.92919559]\n",
      " [18.          0.          0.         ...  0.          0.\n",
      "   3.04291483]\n",
      " [15.          0.          2.         ...  0.          0.\n",
      "   0.11260225]\n",
      " ...\n",
      " [16.          1.          0.         ...  0.          1.\n",
      "   1.14233288]\n",
      " [16.          1.          1.         ...  1.          0.\n",
      "   1.80329676]\n",
      " [16.          1.          0.         ...  0.          1.\n",
      "   2.14001388]]\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 13)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_numpy.shape)\n",
    "ground_truth = torch.from_numpy(ground_truth_numpy)\n",
    "print(ground_truth.isnan().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ground_truth_numpy # Convert tensor to numpy array\n",
    "sensitive_attributes = [1, 2]\n",
    "\n",
    "# Define different bin sizes for each sensitive attribute\n",
    "bin_sizes = {\n",
    "    1: 2,  # Binary attribute\n",
    "    2: 4  # More categories\n",
    "}\n",
    "\n",
    "\n",
    "# Digitize multiple sensitive attributes with different bin sizes\n",
    "X_list = [\n",
    "    np.digitize(data[:, col], bins=np.linspace(data[:, col].min(), data[:, col].max(), bin_sizes[col] + 1))\n",
    "    for col in sensitive_attributes\n",
    "]\n",
    "X = np.column_stack(X_list)\n",
    "#X = np.digitize(data[:,1],bins=np.linspace(data[:,1].min(), data[:,1].max(),1)) \n",
    "Y =np.digitize(Xy['target'],bins=np.linspace(Xy['target'].min(), Xy['target'].max(),4)) \n",
    "Z =  np.digitize(data[:, 7],bins=np.linspace(data[:, 7].min(), data[:, 7].max(), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Mutual Information: 0.005112465952673926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "import numpy as np\n",
    "\n",
    "def conditional_mutual_info(X, Y, Z, min_samples=5):\n",
    "    \"\"\"\n",
    "    Compute Conditional Mutual Information I(X; Y | Z)\n",
    "    using mutual_info_score from sklearn.\n",
    "    \n",
    "    Parameters:\n",
    "    - X, Y, Z: 1D NumPy arrays of the same length (categorical data)\n",
    "    - min_samples: Minimum number of samples required per Z = z\n",
    "    \n",
    "    Returns:\n",
    "    - CMI value\n",
    "    \"\"\"\n",
    "    unique_Z = np.unique(Z)\n",
    "    cmi = 0.0\n",
    "\n",
    "    for z in unique_Z:\n",
    "        mask = Z == z\n",
    "        \n",
    "        # Skip if not enough samples\n",
    "        if np.sum(mask) < min_samples:\n",
    "            continue\n",
    "        \n",
    "        # Skip if X or Y has no variance\n",
    "        if len(np.unique(X[mask])) <= 1 or len(np.unique(Y[mask])) <= 1:\n",
    "            continue\n",
    "\n",
    "        # Compute MI for Z = z, weighted by P(Z = z)\n",
    "        cmi += mutual_info_score(X[mask], Y[mask]) * np.mean(mask)\n",
    "    \n",
    "    return cmi\n",
    "\n",
    "\n",
    "# Compute CMI\n",
    "cmi_value = conditional_mutual_info(X, Y, Z)\n",
    "print(\"Conditional Mutual Information:\", cmi_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.006568645808023421)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries after reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# Load the uploaded student performance data\n",
    "\n",
    "\n",
    "\n",
    "# Modified CMI function to handle multiple sensitive attributes\n",
    "def conditional_mutual_info_multi_sensitive(X_list, Y, Z, min_samples=5):\n",
    "    \"\"\"\n",
    "    Compute CMI for multiple sensitive attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_list: List of sensitive attribute arrays (e.g., [Gender, Ethnicity])\n",
    "    - Y: Target variable (GradeClass)\n",
    "    - Z: Conditioning variable (Parental Support)\n",
    "    - min_samples: Minimum number of samples required for each subgroup\n",
    "    \n",
    "    Returns:\n",
    "    - CMI value\n",
    "    \"\"\"\n",
    "    unique_Z = np.unique(Z)\n",
    "    cmi = 0.0\n",
    "\n",
    "    for z in unique_Z:\n",
    "        mask = Z == z\n",
    "\n",
    "        # Skip if not enough samples\n",
    "        if np.sum(mask) < min_samples:\n",
    "            continue\n",
    "\n",
    "        # Combine sensitive attributes\n",
    "        X_combined = np.column_stack([X[mask] for X in X_list])\n",
    "\n",
    "        # Skip if X or Y has no variance\n",
    "        if np.all([len(np.unique(X_combined[:, i])) <= 1 for i in range(X_combined.shape[1])]) or len(np.unique(Y[mask])) <= 1:\n",
    "            continue\n",
    "\n",
    "        # Compute MI for combined sensitive attributes\n",
    "        mi = mutual_info_score(X_combined[:, 0], Y[mask])  # Using the first sensitive attribute\n",
    "        for i in range(1, X_combined.shape[1]):\n",
    "            mi += mutual_info_score(X_combined[:, i], Y[mask])\n",
    "\n",
    "        weight = np.mean(mask)\n",
    "        cmi += mi * weight\n",
    "\n",
    "    return cmi\n",
    "\n",
    "\n",
    "\n",
    "# Target and conditioning variable\n",
    "\n",
    "\n",
    "# Compute CMI for multiple sensitive attributes\n",
    "cmi_multi_sensitive = conditional_mutual_info_multi_sensitive(X_list, Y, Z)\n",
    "\n",
    "cmi_multi_sensitive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values: 29.96%\n"
     ]
    }
   ],
   "source": [
    "#30% MCAR\n",
    "np.random.seed(42)\n",
    "\n",
    "p_macr30 = 0.301\n",
    "\n",
    "\n",
    "mask_mcar30 = np.random.rand(*ground_truth.shape) < p_macr30  # True for missing values, false for others\n",
    "data_mcar30 = np.copy(ground_truth)\n",
    "\n",
    "# Apply the mask to set the selected values to NaN\n",
    "data_mcar30[mask_mcar30] = np.nan\n",
    "\n",
    "# Convert to a torch tensor\n",
    "data_mcar30 = torch.from_numpy(data_mcar30)\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "missing_values_mcar30 = torch.isnan(data_mcar30)  # Create a boolean tensor where NaNs are True\n",
    "missing_percentage_mcar30 = torch.sum(missing_values_mcar30).item() / data_mcar30.numel() * 100  # Calculate the percentage of NaNs\n",
    "print(f\"Percentage of missing values: {missing_percentage_mcar30:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mcar30_numpy = data_mcar30.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., nan,  0., ..., nan,  1., nan])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mcar30_numpy[0:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7581304589208884\n"
     ]
    }
   ],
   "source": [
    "#sinkhorn\n",
    "n_mcar30, d_mcar30 = data_mcar30.shape\n",
    "batchsize = 128 # If the batch size is larger than half the dataset's size,\n",
    "                # it will be redefined in the imputation methods.\n",
    "lr = 1e-2\n",
    "epsilon_mcar30 = pick_epsilon(data_mcar30)\n",
    "#epsilon_mcar30 = 0.3\n",
    "print(epsilon_mcar30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Iteration 0:\t Loss: 2.0017\t Validation MAE: 1.4362\tRMSE: 2.8862\n",
      "INFO:root:Iteration 50:\t Loss: 2.6822\t Validation MAE: 1.4343\tRMSE: 2.8846\n",
      "INFO:root:Iteration 100:\t Loss: 1.7624\t Validation MAE: 1.4307\tRMSE: 2.8823\n",
      "INFO:root:Iteration 150:\t Loss: 3.0153\t Validation MAE: 1.4283\tRMSE: 2.8825\n",
      "INFO:root:Iteration 200:\t Loss: 3.8440\t Validation MAE: 1.4263\tRMSE: 2.8821\n",
      "INFO:root:Iteration 250:\t Loss: 2.9376\t Validation MAE: 1.4248\tRMSE: 2.8822\n",
      "INFO:root:Iteration 300:\t Loss: 3.1722\t Validation MAE: 1.4226\tRMSE: 2.8813\n",
      "INFO:root:Iteration 350:\t Loss: 2.0711\t Validation MAE: 1.4212\tRMSE: 2.8807\n",
      "INFO:root:Iteration 400:\t Loss: 2.8049\t Validation MAE: 1.4189\tRMSE: 2.8799\n",
      "INFO:root:Iteration 450:\t Loss: 4.9795\t Validation MAE: 1.4176\tRMSE: 2.8798\n",
      "INFO:root:Iteration 500:\t Loss: 3.4460\t Validation MAE: 1.4165\tRMSE: 2.8800\n",
      "INFO:root:Iteration 550:\t Loss: 2.5326\t Validation MAE: 1.4157\tRMSE: 2.8796\n",
      "INFO:root:Iteration 600:\t Loss: 5.1077\t Validation MAE: 1.4168\tRMSE: 2.8801\n",
      "INFO:root:Iteration 650:\t Loss: 1.6647\t Validation MAE: 1.4167\tRMSE: 2.8793\n",
      "INFO:root:Iteration 700:\t Loss: 2.3491\t Validation MAE: 1.4160\tRMSE: 2.8782\n",
      "INFO:root:Iteration 750:\t Loss: 2.3581\t Validation MAE: 1.4164\tRMSE: 2.8778\n",
      "INFO:root:Iteration 800:\t Loss: 2.6576\t Validation MAE: 1.4158\tRMSE: 2.8765\n",
      "INFO:root:Iteration 850:\t Loss: 1.8251\t Validation MAE: 1.4155\tRMSE: 2.8755\n",
      "INFO:root:Iteration 900:\t Loss: 2.4810\t Validation MAE: 1.4151\tRMSE: 2.8756\n",
      "INFO:root:Iteration 950:\t Loss: 1.6432\t Validation MAE: 1.4144\tRMSE: 2.8744\n",
      "INFO:root:Iteration 1000:\t Loss: 2.3645\t Validation MAE: 1.4141\tRMSE: 2.8743\n",
      "INFO:root:Iteration 1050:\t Loss: 2.4533\t Validation MAE: 1.4138\tRMSE: 2.8745\n",
      "INFO:root:Iteration 1100:\t Loss: 2.7222\t Validation MAE: 1.4134\tRMSE: 2.8733\n",
      "INFO:root:Iteration 1150:\t Loss: 2.7252\t Validation MAE: 1.4128\tRMSE: 2.8735\n",
      "INFO:root:Iteration 1200:\t Loss: 2.5236\t Validation MAE: 1.4130\tRMSE: 2.8733\n",
      "INFO:root:Iteration 1250:\t Loss: 1.9977\t Validation MAE: 1.4130\tRMSE: 2.8731\n",
      "INFO:root:Iteration 1300:\t Loss: 2.4858\t Validation MAE: 1.4124\tRMSE: 2.8721\n",
      "INFO:root:Iteration 1350:\t Loss: 2.7525\t Validation MAE: 1.4112\tRMSE: 2.8716\n",
      "INFO:root:Iteration 1400:\t Loss: 3.4974\t Validation MAE: 1.4116\tRMSE: 2.8713\n",
      "INFO:root:Iteration 1450:\t Loss: 1.6687\t Validation MAE: 1.4118\tRMSE: 2.8708\n",
      "INFO:root:Iteration 1500:\t Loss: 1.4714\t Validation MAE: 1.4114\tRMSE: 2.8709\n",
      "INFO:root:Iteration 1550:\t Loss: 1.5945\t Validation MAE: 1.4116\tRMSE: 2.8703\n",
      "INFO:root:Iteration 1600:\t Loss: 2.6685\t Validation MAE: 1.4121\tRMSE: 2.8703\n",
      "INFO:root:Iteration 1650:\t Loss: 2.9729\t Validation MAE: 1.4121\tRMSE: 2.8692\n",
      "INFO:root:Iteration 1700:\t Loss: 3.2063\t Validation MAE: 1.4107\tRMSE: 2.8676\n",
      "INFO:root:Iteration 1750:\t Loss: 4.7336\t Validation MAE: 1.4108\tRMSE: 2.8678\n",
      "INFO:root:Iteration 1800:\t Loss: 2.5255\t Validation MAE: 1.4100\tRMSE: 2.8667\n",
      "INFO:root:Iteration 1850:\t Loss: 1.8096\t Validation MAE: 1.4089\tRMSE: 2.8657\n",
      "INFO:root:Iteration 1900:\t Loss: 2.0559\t Validation MAE: 1.4099\tRMSE: 2.8674\n",
      "INFO:root:Iteration 1950:\t Loss: 4.4723\t Validation MAE: 1.4095\tRMSE: 2.8672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.410387068381239 2.8678257103720632\n"
     ]
    }
   ],
   "source": [
    "sk_imputer_mcar30 = SinkhornImputation_CMI(eps=epsilon_mcar30, batchsize=batchsize, lr=lr, niter=2000)\n",
    "sk_imp_mcar30, sk_maes30mcar, sk_rmses30mcar,sk_loss_mcar30 = sk_imputer_mcar30.fit_transform(data_mcar30, verbose=True, report_interval=50, X_true=ground_truth)\n",
    "#using numpy version of data\n",
    "sk_imp_mcar30_numpy = sk_imp_mcar30.detach().cpu().numpy()\n",
    "\n",
    "sk_mae_mcar30 = MAE(sk_imp_mcar30_numpy,ground_truth_numpy ,mask_mcar30)\n",
    "sk_rmse_mcar30 = RMSE(sk_imp_mcar30_numpy, ground_truth_numpy,mask_mcar30)\n",
    "print(sk_mae_mcar30,sk_rmse_mcar30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17.0000,  1.0000,  0.0000,  ...,  0.0321,  0.0000,  2.9292],\n",
       "        [16.4177,  0.5797,  0.8860,  ...,  0.0000,  0.0000,  3.0429],\n",
       "        [16.5306,  0.0000,  2.0000,  ...,  0.0000,  0.1719,  0.1126],\n",
       "        ...,\n",
       "        [16.0000,  0.6995,  0.0000,  ...,  0.0000,  1.0000,  1.1423],\n",
       "        [16.0000,  1.0000,  0.7378,  ...,  1.0000,  0.0000,  1.7813],\n",
       "        [16.0000,  0.6411,  0.0000,  ...,  0.3265,  1.0000,  2.2147]],\n",
       "       grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_imp_mcar30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = sk_imp_mcar30.detach().numpy()  # Convert tensor to numpy array\n",
    "sensitive_attributes = [1, 2]\n",
    "\n",
    "# Define different bin sizes for each sensitive attribute\n",
    "bin_sizes = {\n",
    "    1: 2,  # Binary attribute\n",
    "    2: 4  # More categories\n",
    "}\n",
    "\n",
    "\n",
    "# Digitize multiple sensitive attributes with different bin sizes\n",
    "X_list = [\n",
    "    np.digitize(data[:, col], bins=np.linspace(data[:, col].min(), data[:, col].max(), bin_sizes[col] + 1))\n",
    "    for col in sensitive_attributes\n",
    "]\n",
    "X = np.column_stack(X_list)\n",
    "#X = np.digitize(data[:,1],bins=np.linspace(data[:,1].min(), data[:,1].max(),2)) \n",
    "Y =np.digitize(Xy['target'],bins=np.linspace(Xy['target'].min(), Xy['target'].max(),4)) \n",
    "Z =  np.digitize(data[:, 7],bins=np.linspace(data[:, 7].min(), data[:, 7].max(), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Mutual Information: 0.0024563764211416213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def conditional_mutual_info(X, Y, Z):\n",
    "    unique_Z = np.unique(Z)\n",
    "    cmi = 0.0\n",
    "    for z in unique_Z:\n",
    "        mask = Z == z\n",
    "        cmi += mutual_info_score(X[mask], Y[mask]) * np.mean(mask)\n",
    "    return cmi\n",
    "\n",
    "cmi_value = conditional_mutual_info(X, Y, Z)\n",
    "print(\"Conditional Mutual Information:\", cmi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique X: [1 2]\n",
      "Unique Y: [1 2 3 4]\n",
      "Unique Z: [1 2 3 4]\n",
      "Mutual Information (unconditional): 0.0005774896490555737\n",
      "Z = 1:\n",
      "X subset: [1 1 1 2 1 1 1 1 1 2 1 2 1 1 1 1 2 2 1 1 1 1 1 2 2 1 1 2 2 2 2 2 1 2 1 2 1\n",
      " 1 2 2 2 2 1 2 1 2 1 2 2 2 1 1 2 1 1 1 1 1 2 1 2 1 2 1 1 1 1 1 2 2 1 1 2 1\n",
      " 1 2 2 2 1 1 2 2 1 1 1 2 1 2 2 1 1 1 1 1 1 1 2 2 1 1 2 1 1 2 1 1 1 1 2 1 2\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 2 1 2 1 1 1 1 2 2 2 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2 1 1 2 1 2 2 1 1 2\n",
      " 1 2 1 1 2 2 1 2 2 1 2 2 1 2 1 1 1 2 2 1 1 2 2 1 2 2 1 1 2 2 1 2 2 2 1 2 1\n",
      " 1 1 1 2 1 1 2 1 1 1 1 2 2 1 2 1 1 2 1 1 1 1 1 1 2 2 2 1 2 1 2 1 2 1 2 2 1\n",
      " 2 1 1 2 2 1 1 2 1 1 1 1 2 2 1 2 1 2 2 1 1 1 2 2 2 1 1 1 1 1 1 2 1 1 2 1 1\n",
      " 1 1 1 1 1 1 1 2 2 2 2 1 1 2 1 1 2 2 1 1 1 2 2 2 2 2 1 2 1 1 2 1 2 1 1 1 1\n",
      " 1 2 1 1 1 1 2 2 1 1 1 1 1 2 1 1 2 2 2 1 2 2 2 2 2 2 2 1 1 2 1 2 1 1 1 1 2\n",
      " 1 1 1 1 1 1 2 1 1 2 2 2 1 2 1 1 1 1 1 2 1 2 1 1 2 2 1 1 2 1 1 2 2 2 2 2 2\n",
      " 2 1 2 2 1 2 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1\n",
      " 1 2 1 2 1 2 1 2 1 1 2 1 2 1 1 1 2 1 1 2 2 1 1 2 1 2]\n",
      "Y subset: [1 3 4 4 4 4 4 4 4 1 2 4 4 4 4 4 2 4 4 4 4 2 3 3 4 4 2 4 4 2 4 2 1 4 4 4 4\n",
      " 4 2 4 4 2 4 2 2 4 1 3 4 3 4 3 4 4 4 3 2 4 4 4 4 4 4 3 2 4 4 4 3 4 3 2 4 4\n",
      " 4 4 4 4 4 4 4 2 4 2 4 4 4 2 4 4 4 4 3 2 4 4 2 4 4 4 1 4 4 3 3 4 3 3 4 1 4\n",
      " 3 3 4 4 4 3 4 4 4 4 4 4 4 3 4 4 4 2 4 4 2 3 3 4 3 4 2 3 4 4 4 4 4 3 3 4 4\n",
      " 3 3 4 2 2 2 1 4 4 3 4 4 4 2 3 4 3 4 4 4 2 1 3 4 4 4 4 2 4 3 1 2 2 4 4 3 4\n",
      " 4 1 4 4 4 4 2 4 4 4 3 3 4 2 4 3 2 3 4 3 4 3 2 3 4 4 1 4 3 4 4 3 3 4 3 3 2\n",
      " 3 1 4 4 4 4 4 3 4 4 4 4 3 1 4 4 4 4 4 4 3 4 3 4 4 4 1 4 1 4 4 4 4 2 2 4 4\n",
      " 4 4 4 4 4 4 1 2 2 4 3 4 2 4 2 2 2 1 2 3 4 4 4 1 4 4 2 4 4 4 3 4 4 2 4 4 2\n",
      " 1 4 4 4 4 4 1 4 4 4 4 4 4 3 1 3 2 2 2 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 2 3 4\n",
      " 4 4 3 4 2 4 4 4 4 4 1 4 3 4 2 4 4 4 4 4 2 4 2 2 2 2 3 3 4 4 3 4 4 3 2 4 4\n",
      " 4 4 3 4 4 2 2 2 1 4 2 1 4 3 2 4 4 4 4 3 4 1 4 1 3 4 4 4 4 4 4 3 4 3 4 4 4\n",
      " 4 4 2 4 3 4 4 4 4 2 1 2 4 4 4 3 1 4 3 3 2 4 4 3 3 3 2 4 3 3 4 4 3 4 1 1 3\n",
      " 1 1 1 3 1 4 3 2 1 2 4 3 2 1 2 2 2 3 4 1 1 2 3 4 1 1]\n",
      "Mutual Info: 0.0021401377750850242\n",
      "Z = 2:\n",
      "X subset: [2 1 1 ... 1 2 1]\n",
      "Y subset: [2 4 1 ... 2 1 1]\n",
      "Mutual Info: 0.0010783501021579822\n",
      "Z = 3:\n",
      "X subset: [1 2 1 1 2 2 1 1 2 1 1 2 1 1 1 2 2 1 2 1 2 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 2 1 2 1 1 1 2 1 2 1 1 2 2 1 2 1 1 2 1 1 2 1 2 1 1 1 2 2 2 1 2 1 1 2 1 1 1\n",
      " 1 2 1 2 1 1 1 1 1 2 1 2 1 2 1 2 2 1 1 1 2 1 1 1 2 1 1 1 2 1 2 2 2 1 2 2 1\n",
      " 2 1 1 1 1 1 1 1 1 2 2 1 2 2 2 2 1 1 1 1 1 1 2 2 1 2 2 2 1 2 1 2 1 2 1 1 2\n",
      " 1 1 1 1 1 1 1 1 2 1 1 1 2 2 1 2 1 2 2 2 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 1 1 2 1 1 2 2 1 1 2 2 1 2 2 2 2 2 1 1 2 1 2 1 2 1 1 1 1 1 2 1 1\n",
      " 2 1 2 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 2 1 2 1 1 1 2 2 2 1 1 1 2 1 1 1 1 2\n",
      " 1 1 2 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 2 2 1 1 1 1 1 1 2 2 1 2 1 1 1 2 2 1 1 1 2 1 1 1 2 1 2 1 2 1 1 1 1 1 2 1 2\n",
      " 1 2 2 1 1 1 1 2 1 2 1 2 1 1 1 1 2 1 1 2 2 2 1 2 2 2 2 1 2 2 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 2 1 1 2 2 1 1 2 1 1 1 1 1 2 1 2 2 2 2 1 1 2 1 1 1 1 1\n",
      " 2 1 1 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 2 2 1 2 1 1 2 1 2\n",
      " 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 1 1 2 1 1 1 1 2 1 2 1 2 1 1 1 1 2 2 2 1 2 1\n",
      " 1 1 2 2 2 1 2 2]\n",
      "Y subset: [3 4 2 4 2 3 4 3 2 4 2 4 3 4 4 3 3 1 4 4 4 3 1 1 3 2 1 4 2 3 2 2 4 1 4 1 2\n",
      " 3 3 4 1 2 4 2 4 4 4 1 3 4 4 4 2 1 2 1 1 3 4 4 4 4 2 4 2 4 4 4 4 4 4 1 3 4\n",
      " 4 1 4 1 4 3 4 3 2 4 2 4 4 4 3 4 3 4 4 3 4 1 1 3 4 2 1 2 2 2 4 3 4 3 4 3 3\n",
      " 4 4 1 3 3 1 4 2 4 1 1 1 3 2 4 4 2 3 4 4 4 4 4 2 4 4 2 4 4 2 1 4 2 4 4 4 4\n",
      " 1 4 3 2 4 2 4 3 2 4 4 1 4 3 4 1 4 3 2 3 1 4 4 2 1 3 4 3 3 3 2 4 4 3 1 4 4\n",
      " 4 3 4 2 4 1 4 4 1 1 3 4 4 3 1 2 4 4 2 4 1 4 2 4 3 1 2 4 4 4 4 3 4 2 4 4 4\n",
      " 4 4 4 3 4 4 3 1 2 4 1 4 3 3 4 4 2 4 3 2 1 4 4 4 4 3 4 4 3 2 4 3 3 4 1 1 4\n",
      " 3 1 4 4 1 4 4 2 3 1 1 4 4 4 2 3 1 2 1 3 3 1 2 1 4 4 1 1 2 4 4 1 3 4 2 4 2\n",
      " 2 3 1 2 4 4 2 4 4 1 2 1 3 3 4 4 1 4 2 1 4 1 4 4 4 4 4 2 3 3 3 3 2 4 3 4 4\n",
      " 4 1 4 3 4 4 4 1 3 1 4 3 4 2 3 2 3 1 4 4 4 4 4 1 4 2 4 4 2 4 2 4 2 2 2 4 4\n",
      " 4 3 4 1 4 4 2 4 3 4 1 4 3 2 3 1 1 3 4 1 3 1 3 1 3 4 4 4 2 4 4 1 2 4 4 4 4\n",
      " 3 3 4 4 4 4 2 2 1 4 4 4 4 3 4 3 4 4 1 4 4 4 4 2 4 4 2 4 3 1 1 2 4 4 4 4 1\n",
      " 4 1 2 4 1 4 2 1 1 4 1 1 1 3 1 4 1 4 2 1 4 2 1 4 1 3 1 1 1 2 1 3 3 1 4 2 2\n",
      " 3 3 2 1 1 1 3 4]\n",
      "Mutual Info: 0.0008789171688789732\n",
      "Z = 4:\n",
      "X subset: [1 1 2 2 1 1 2 1 2 2 1 2 1 2 1 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1\n",
      " 1 2 1 1 2 1 1 2 2 1 1 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 1 2 1 2 2 2 1 2 2 2 1\n",
      " 1 2 1 2 1 1 1 2 1 1 1 1 2 1 1 2 2 1 1 2 1 1 2 2 2 2 2 1 1 2 1 1 2 1 1 2 1\n",
      " 1 1 1 1 2 2 1 1 1 2 1 1 2 2 1 2 1 2 1 1 2 2 1 1 2 1 2 2 1 1 1 2 2 2 2 1 2\n",
      " 1 1 1 2 1 2 2 2 2 1 2 1 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 2 2 1 1 1 1 2 1 2 1]\n",
      "Y subset: [4 4 2 4 1 1 4 2 1 4 2 4 3 1 3 3 3 4 3 4 4 1 4 4 3 1 4 2 4 2 2 1 1 4 2 2 2\n",
      " 3 1 4 4 4 1 2 4 4 1 2 2 4 2 3 3 4 3 4 2 4 4 3 4 2 4 3 4 4 1 4 2 3 3 1 4 1\n",
      " 3 4 4 4 3 1 4 1 4 4 3 2 4 4 4 4 4 1 2 4 1 4 4 4 1 4 4 4 1 4 3 4 2 1 1 4 2\n",
      " 4 3 3 3 4 4 4 1 1 1 4 4 4 4 4 3 4 2 3 2 1 3 2 4 4 4 2 1 2 2 1 2 4 4 4 4 3\n",
      " 4 4 2 3 3 4 2 1 3 1 1 1 2 4 3 4 4 4 4 2 1 1 1 3 1 4 1 1 4 3 1 3 1 1 3 1 4]\n",
      "Mutual Info: 0.016725493091923338\n",
      "Conditional Mutual Information: 0.0024563764211416213\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "\n",
    "\n",
    "# Debugging: Check unique values\n",
    "print(\"Unique X:\", np.unique(X))\n",
    "print(\"Unique Y:\", np.unique(Y))\n",
    "print(\"Unique Z:\", np.unique(Z))\n",
    "\n",
    "# Debugging: Check unconditional MI\n",
    "mi = mutual_info_score(X, Y)\n",
    "print(\"Mutual Information (unconditional):\", mi)\n",
    "\n",
    "# Debugging: Check subsets for each Z\n",
    "for z in np.unique(Z):\n",
    "    mask = Z == z\n",
    "    print(f\"Z = {z}:\")\n",
    "    print(\"X subset:\", X[mask])\n",
    "    print(\"Y subset:\", Y[mask])\n",
    "    print(\"Mutual Info:\", mutual_info_score(X[mask], Y[mask]))\n",
    "\n",
    "# Compute CMI\n",
    "def conditional_mutual_info(X, Y, Z):\n",
    "    unique_Z = np.unique(Z)\n",
    "    cmi = 0.0\n",
    "    for z in unique_Z:\n",
    "        mask = Z == z\n",
    "        if np.sum(mask) > 1 and len(np.unique(X[mask])) > 1 and len(np.unique(Y[mask])) > 1:\n",
    "            mi = mutual_info_score(X[mask], Y[mask])\n",
    "            cmi += mi * np.mean(mask)\n",
    "    return cmi\n",
    "\n",
    "cmi_value = conditional_mutual_info(X, Y, Z)\n",
    "\n",
    "print(\"Conditional Mutual Information:\", cmi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.016868525956684213)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries after reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# Load the uploaded student performance data\n",
    "\n",
    "\n",
    "\n",
    "# Modified CMI function to handle multiple sensitive attributes\n",
    "def conditional_mutual_info_multi_sensitive(X_list, Y, Z, min_samples=5):\n",
    "    \"\"\"\n",
    "    Compute CMI for multiple sensitive attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_list: List of sensitive attribute arrays (e.g., [Gender, Ethnicity])\n",
    "    - Y: Target variable (GradeClass)\n",
    "    - Z: Conditioning variable (Parental Support)\n",
    "    - min_samples: Minimum number of samples required for each subgroup\n",
    "    \n",
    "    Returns:\n",
    "    - CMI value\n",
    "    \"\"\"\n",
    "    unique_Z = np.unique(Z)\n",
    "    cmi = 0.0\n",
    "\n",
    "    for z in unique_Z:\n",
    "        mask = Z == z\n",
    "\n",
    "        # Skip if not enough samples\n",
    "        if np.sum(mask) < min_samples:\n",
    "            continue\n",
    "\n",
    "        # Combine sensitive attributes\n",
    "        X_combined = np.column_stack([X[mask] for X in X_list])\n",
    "\n",
    "        # Skip if X or Y has no variance\n",
    "        if np.all([len(np.unique(X_combined[:, i])) <= 1 for i in range(X_combined.shape[1])]) or len(np.unique(Y[mask])) <= 1:\n",
    "            continue\n",
    "\n",
    "        # Compute MI for combined sensitive attributes\n",
    "        mi = mutual_info_score(X_combined[:, 0], Y[mask])  # Using the first sensitive attribute\n",
    "        for i in range(1, X_combined.shape[1]):\n",
    "            mi += mutual_info_score(X_combined[:, i], Y[mask])\n",
    "\n",
    "        weight = np.mean(mask)\n",
    "        cmi += mi * weight\n",
    "\n",
    "    return cmi\n",
    "\n",
    "\n",
    "\n",
    "# Target and conditioning variable\n",
    "\n",
    "\n",
    "# Compute CMI for multiple sensitive attributes\n",
    "cmi_multi_sensitive = conditional_mutual_info_multi_sensitive(X_list, Y, Z)\n",
    "\n",
    "cmi_multi_sensitive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Mutual Information (CMI) for multiple sensitive attributes: 0.05089958732674174\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "\n",
    "\n",
    "# Function to compute joint entropy\n",
    "def joint_entropy(*args, bins=10):\n",
    "    \"\"\"\n",
    "    Compute joint entropy of multiple variables.\n",
    "    \n",
    "    Parameters:\n",
    "    - args: Arrays of variables (e.g., X, Y, Z)\n",
    "    - bins: Number of bins for histogram (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "    - Joint entropy value\n",
    "    \"\"\"\n",
    "    # Stack variables and compute histogram\n",
    "    hist, _ = np.histogramdd(np.column_stack(args), bins=bins)\n",
    "    prob = hist / np.sum(hist)  # Normalize to get probabilities\n",
    "    prob = prob[prob > 0]  # Remove zero probabilities\n",
    "    return entropy(prob, base=2)\n",
    "\n",
    "def conditional_mutual_info_multi_sensitive(X, Y, Z, min_samples=5, bins=10):\n",
    "    \"\"\"\n",
    "    Compute Conditional Mutual Information (CMI) for multiple sensitive attributes.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_list: List of sensitive attribute arrays (e.g., [Gender, Ethnicity])\n",
    "    - Y: Target variable (e.g., GradeClass)\n",
    "    - Z: Conditioning variable (e.g., Parental Support)\n",
    "    - min_samples: Minimum number of samples required for each subgroup\n",
    "    - bins: Number of bins for joint entropy calculation\n",
    "    \n",
    "    Returns:\n",
    "    - CMI value\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    \n",
    "    unique_Z = np.unique(Z)\n",
    "    cmi = 0.0\n",
    "\n",
    "    for z in unique_Z:\n",
    "        mask = (Z == z)\n",
    "\n",
    "        # Skip if not enough samples\n",
    "        if np.sum(mask) < min_samples:\n",
    "            continue\n",
    "\n",
    "        # Apply the mask to all sensitive attributes at once\n",
    "        X_masked = X[mask]\n",
    "        Y_masked = Y[mask]\n",
    "\n",
    "        # Skip if X or Y has no variance\n",
    "        if np.all([len(np.unique(X_masked[:, i])) <= 1 for i in range(X_masked.shape[1])]) or len(np.unique(Y_masked)) <= 1:\n",
    "            continue\n",
    "\n",
    "        # Compute MI using corrected joint entropy formula\n",
    "        H_XZ = joint_entropy(X_masked, Z[mask])  # Fixed term\n",
    "        H_YZ = joint_entropy(Y_masked, Z[mask])\n",
    "        H_XYZ = joint_entropy(X_masked, Y_masked, Z[mask])\n",
    "        H_Z = joint_entropy(Z[mask])  # Corrected term\n",
    "        mi = (H_XZ + H_YZ - H_XYZ - H_Z)  # Fixed MI formula\n",
    "        \n",
    "        weight = np.mean(mask)\n",
    "        cmi += mi * weight\n",
    "    \n",
    "    return max(0, cmi) \n",
    "\n",
    "# Define sensitive attributes (e.g., Gender and Ethnicity)\n",
    "\n",
    "\n",
    "cmi_multi_sensitive = conditional_mutual_info_multi_sensitive(X, Y, Z)\n",
    "print(\"Conditional Mutual Information (CMI) for multiple sensitive attributes:\", cmi_multi_sensitive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_mutual_information(data, X, Y, Z, delta=1):\n",
    "\n",
    "  cmi = 0\n",
    "\n",
    "  P_Z = data.groupby(Z).size()\n",
    "  P_Z = P_Z / P_Z.sum()\n",
    "\n",
    "  P_XZ = data.groupby(X + Z).size()\n",
    "  P_XZ = P_XZ / P_XZ.sum()\n",
    "\n",
    "  P_YZ = data.groupby(Y + Z).size()\n",
    "  P_YZ = P_YZ / P_YZ.sum()\n",
    "\n",
    "  P_XYZ = data.groupby(X + Y + Z).size()\n",
    "  P_XYZ = P_XYZ / P_XYZ.sum()\n",
    "\n",
    "  for ind in P_XYZ.index:\n",
    "    x_ind = ind[:len(X)]\n",
    "    y_ind = ind[len(X):len(X + Y)]\n",
    "    z_ind = ind[len(X + Y):]\n",
    "\n",
    "    xz_ind = x_ind + z_ind\n",
    "    yz_ind = y_ind + z_ind\n",
    "    xyz_ind = ind\n",
    "\n",
    "    z_ind = pd.MultiIndex.from_tuples([z_ind], names=Z) if len(Z) != 1 else pd.Index(z_ind, name=Z[0])\n",
    "    xz_ind = pd.MultiIndex.from_tuples([xz_ind], names=X + Z)\n",
    "    yz_ind = pd.MultiIndex.from_tuples([yz_ind], names=Y + Z)\n",
    "    xyz_ind = pd.MultiIndex.from_tuples([xyz_ind], names=X + Y + Z)\n",
    "\n",
    "    cmi += delta * P_XYZ[xyz_ind].item() * np.log2(\n",
    "      P_Z[z_ind].item() * P_XYZ[xyz_ind].item() / (P_XZ[xz_ind].item() * P_YZ[yz_ind].item()))\n",
    "\n",
    "  return cmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_value =conditional_mutual_information(df, X, Y, Z, delta=1)\n",
    "print(\"Conditional Mutual Information:\", cmi_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
